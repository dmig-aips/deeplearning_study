{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, rnn, utils as gutils\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import mxnet as mx\n",
    "import pandas as pd\n",
    "import collections\n",
    "import random\n",
    "import tarfile\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open(\"/home/mj/naver_movie_sentiment/ratings_train.txt\", 'r') as fp:\n",
    "    for line in fp:\n",
    "        ids, doc, label = line.split(\"\\t\")\n",
    "        train_data.append([doc, label.strip()])\n",
    "\n",
    "train_data = pd.DataFrame(train_data[1:], columns=['Doc', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "with open(\"/home/mj/naver_movie_sentiment/ratings_test.txt\", 'r') as fp:\n",
    "    for line in fp:\n",
    "        ids, doc, label = line.split(\"\\t\")\n",
    "        test_data.append([doc, label.strip()])\n",
    "\n",
    "test_data = pd.DataFrame(test_data[1:], columns=['Doc', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_corpus(corpus):\n",
    "    def tokenize(sent):\n",
    "        return sent.split(\" \")\n",
    "    return [tokenize(review) for review in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(corpus):\n",
    "    tokenized_data = get_tokenized_corpus(corpus)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return text.vocab.Vocabulary(counter, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('# Words in vocab:', 24005)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = get_vocab(train_data['Doc'])\n",
    "'# Words in vocab:', len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(data, vocab):\n",
    "    max_l = 200\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "    \n",
    "    tokenized_data = get_tokenized_corpus(data['Doc'])\n",
    "    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])\n",
    "    labels = [int(s) for s in data['label']]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set = gdata.ArrayDataset(*preprocess_corpus(train_data, vocab))\n",
    "test_set = gdata.ArrayDataset(*preprocess_corpus(test_data, vocab))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (128, 200) y (128,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 1172)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Construct RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Block):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        # embedding matrix\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        \n",
    "        self.Encoder = rnn.LSTM(num_hiddens, num_layers=num_layers, bidirectional=True, input_size=embed_size)\n",
    "        self.Decoder = nn.Dense(2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        \n",
    "        states = self.Encoder(embeddings)\n",
    "        \n",
    "        encoding = nd.concat(states[0], states[-1])\n",
    "        outputs = self.Decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "if len(mx.test_utils.list_gpus()) != 0:\n",
    "    ctx=mx.gpu()\n",
    "else:\n",
    "    ctx=mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 300, 1\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)\n",
    "net.initialize(init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "trainer = gluon.Trainer(net.collect_params(),'adam', {'learning_rate' : lr})\n",
    "loss_fn = gloss.SoftmaxCrossEntropyLoss()\n",
    "eval_metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f196ed5307847efb96e4e825550d707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=1172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 1, loss : 0.481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996ad0166249454f90e1b1d56f439bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validation', max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 1, acc : 0.776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db814a43a4a74080aab9f1b43ddaecaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=1172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 2, loss : 0.371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ed22aef8664febb448c95a64cb755c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validation', max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 2, acc : 0.776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0a6154ceb44684bf99922e236bc8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=1172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 3, loss : 0.320\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e10e654651445e93f450c8b463cbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validation', max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 3, acc : 0.775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a814d7a02824cdfa0137e565278357c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=1172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 4, loss : 0.276\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4c69bdcf5d4bbaa7f9e94af7f9fb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validation', max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 4, acc : 0.773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30188a9435e40da86fa011a68ddfac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=1172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 5, loss : 0.242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd55a87324848eba298720dfe6aecac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validation', max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch : 5, acc : 0.770\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # training\n",
    "    loss_sum, n = 0.0, 0\n",
    "    for batch in tqdm_notebook(train_iter, desc='training'):\n",
    "        tokens, label = [data.as_in_context(ctx) for data in batch]\n",
    "        \n",
    "        with autograd.record():\n",
    "            pred = net(tokens)\n",
    "            loss = loss_fn(pred, label)\n",
    "            loss.backward()\n",
    "        \n",
    "        trainer.step(batch_size=batch_size)\n",
    "        loss_sum += loss.sum().asscalar()\n",
    "        n += loss.size\n",
    "    print('epoch : %d, loss : %.3f' %(epoch+1, loss_sum / n))\n",
    "    \n",
    "    # validation\n",
    "    for batch in tqdm_notebook(test_iter, desc='validation'):\n",
    "        tokens, label = [data.as_in_context(ctx) for data in batch]\n",
    "        pred = net(tokens)\n",
    "        eval_metric.update(preds=pred, labels=label)\n",
    "    print('epoch : %d, acc : %.3f' %(epoch+1, eval_metric.get()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    sentence = nd.array(vocab.to_indices(sentence), ctx=d2l.try_gpu())\n",
    "    label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)\n",
    "    return 'positive' if label.asscalar() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = '이 영화는 아벤져수인지 어벤져스인지 하는 미제앞잡이 벌레 무리들에 대한 영화가 아닌 \\\n",
    "우주를 진정으로 사랑하는 마음씨와 노블리스 오블리쥬의 정신을 가진 퓨어블러드 타노스가\\\n",
    "선지자의 깨우침을 알지 못 한 우매하고 멍청한 어벤져스의 방해를 물리치고\\\n",
    "자신의 피와 땀을 바쳐 우주의 평화를 이룩하려 했던 타노스의 두번째 단독 영화이다'\n",
    "review = review.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
