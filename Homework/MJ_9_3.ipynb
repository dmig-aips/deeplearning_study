{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.gluon import rnn, nn\n",
    "import d2l\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDecoder(d2l.Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n",
    "        self.attention_cell = d2l.MLPAttention(num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Dense(vocab_size, flatten = False)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_len, *args):\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        # Transpose outputs to (batch_size, seq_len, hidden_size)\n",
    "        return (outputs.swapaxes(0,1), hidden_state, enc_valid_len)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, hidden_state, enc_valid_len = state\n",
    "        X = self.embedding(X).swapaxes(0, 1)\n",
    "        outputs = []\n",
    "        for x in X:\n",
    "            query = hidden_state[0][-1].expand_dims(axis=1)\n",
    "            context = self.attention_cell(\n",
    "                query, enc_outputs, enc_outputs, enc_valid_len)\n",
    "\n",
    "            x = nd.concat(context, x.expand_dims(axis=1), dim=-1)\n",
    "\n",
    "            out, hidden_state = self.rnn(x.swapaxes(0, 1), hidden_state)\n",
    "            outputs.append(out)\n",
    "        outputs = self.dense(nd.concat(*outputs, dim=0))\n",
    "        return outputs.swapaxes(0, 1), [enc_outputs, hidden_state, enc_valid_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading fra-eng.zip from http://www.manythings.org/anki/fra-eng.zip...\n"
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 1, 0.0\n",
    "batch_size, num_examples, max_len = 32, 1e4, 20\n",
    "lr, num_epochs, ctx = 0.005, 20, mx.cpu()\n",
    "\n",
    "src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(batch_size, max_len, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1143, 1389)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_vocab), len(tgt_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 0.113, time 361.4 sec\n",
      "epoch 10, loss 0.078, time 297.9 sec\n",
      "epoch 15, loss 0.061, time 289.5 sec\n",
      "epoch 20, loss 0.050, time 283.8 sec\n"
     ]
    }
   ],
   "source": [
    "encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "decoder = Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "model = d2l.EncoderDecoder(encoder, decoder)\n",
    "d2l.train_ch7(model, train_iter, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am hungry ! -> j'ai faim !\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I am hungry !\"\n",
    "sentence + ' -> ' + d2l.translate_ch7(model, sentence, src_vocab, tgt_vocab, max_len, ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
