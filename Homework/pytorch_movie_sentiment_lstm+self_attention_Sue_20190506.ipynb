{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_movie_sentiment_lstm+self_attention_Sue.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmnXEmRYptgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from mxnet import autograd, gluon, init, nd\n",
        "# from mxnet.contrib import text\n",
        "# from mxnet.gluon import data as gdata, loss as gloss, nn, rnn, utils as gutils\n",
        "# from tqdm import tqdm_notebook\n",
        "\n",
        "# import mxnet as mx\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "import tarfile\n",
        "import time\n",
        "#!pip install numpy==1.16.1\n",
        "#!pip install mxnet-cu100mkl -- google colab에서 mxnet사용하려면 해당 버전 다운 필요\n",
        "#!pip install gensim -- 넘파이 1.16버전 사용해야함\n",
        "import numpy as np\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FNWk0TDuYkl",
        "colab_type": "code",
        "outputId": "67a32a9e-ba23-4f70-c06b-c5677770b96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.3.0.post4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVmqEj4U2BP-",
        "colab_type": "code",
        "outputId": "fe45ddba-c5e4-4912-f9d4-a403e6064670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "!{sys.executable} -m pip install torchtext==0.2.3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (1.24.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (3.0.4)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Building wheel for torchtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiJ8H_aB1dMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from torchtext.data import TabularDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75XbPGIEKKsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Iterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewsvwp8AuaAT",
        "colab_type": "code",
        "outputId": "d62c662e-279a-42ce-e8e4-f860a3b756b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWCk_ErG-20j",
        "colab_type": "text"
      },
      "source": [
        "#   import library \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-GI9k95lETT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from torch.autograd import Variable "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V26wFd2k_BwD",
        "colab_type": "text"
      },
      "source": [
        "# Load_dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3EnVK8wDKrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(test_sen=None):\n",
        "\n",
        "    \"\"\"\n",
        "    tokenizer : Breaks sentences into a list of words. If sequential=False, no tokenization is applied\n",
        "    Field : A class that stores information about the way of preprocessing\n",
        "    fix_length : An important property of TorchText is that we can let the input to be variable length, and TorchText will\n",
        "                 dynamically pad each sequence to the longest sequence in that \"batch\". But here we are using fi_length which\n",
        "                 will pad each sequence to have a fix length of 200.\n",
        "                 \n",
        "    build_vocab : It will first make a vocabulary or dictionary mapping all the unique words present in the train_data to an\n",
        "                  idx and then after it will use GloVe word embedding to map the index to the corresponding word embedding.\n",
        "                  \n",
        "    vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim) containing the pre-trained word embeddings.\n",
        "    BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    tokenize = lambda x: x.split()\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
        "    LABEL = data.Field(sequential=False, use_vocab=False,   \n",
        "              preprocessing = lambda x: 1 if x == \"pos\" else 0,  \n",
        "              batch_first=True)\n",
        "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
        "    LABEL.build_vocab(train_data)\n",
        "\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
        "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
        "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
        "\n",
        "    train_data, valid_data = train_data.split() # Further splitting of training_data to create new training_data & validation_data\n",
        "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
        "\n",
        "    '''Alternatively we can also use the default configurations'''\n",
        "    # train_iter, test_iter = datasets.IMDB.iters(batch_size=32)\n",
        "\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "\n",
        "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXrp5M_02K55",
        "colab_type": "code",
        "outputId": "f068a3de-4a96-431f-df8a-88c4e6d2ddf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_dataset()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 38.9MB/s]\n",
            ".vector_cache/glove.6B.zip: 862MB [01:27, 9.83MB/s]                           \n",
            "100%|█████████▉| 399561/400000 [00:38<00:00, 10392.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of Text Vocabulary: 251639\n",
            "Vector size of Text Vocabulary:  torch.Size([251639, 300])\n",
            "Label Length: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzoZAmt3CjtQ",
        "colab_type": "text"
      },
      "source": [
        "## one batch example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRQo1S6a_O29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "70e10765-f0c7-4afa-c120-616ef23cdb72"
      },
      "source": [
        "for i,train in enumerate(train_iter):\n",
        "  if i == 0:\n",
        "    print(train.text)\n",
        "    print(train.label)\n",
        "    print(len(train)) \n",
        "    aa = train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[  1543,   3067,      7,  ...,      1,      1,      1],\n",
            "        [  3823,    113,     12,  ...,      1,      1,      1],\n",
            "        [   896,     47,      2,  ...,      1,      1,      1],\n",
            "        ...,\n",
            "        [106556,  17922,   3743,  ...,      1,      1,      1],\n",
            "        [    12,      7,     23,  ...,  40733,   1300,      6],\n",
            "        [  1551,     94,    855,  ...,     44,    168,    531]]), tensor([178, 126, 145,  44, 143, 101, 132, 162, 200, 200, 133, 200, 193, 200,\n",
            "         52, 146, 200, 114, 162, 200,  92, 200,  70, 184, 152, 141, 200, 200,\n",
            "        174, 169, 200, 200]))\n",
            "tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1])\n",
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NU23BYx_8jF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "947e242c-daaa-48ab-b977-86a890a8e3a8"
      },
      "source": [
        "len(TEXT.vocab.itos)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "251639"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItESOPhFCxlk",
        "colab_type": "text"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bLC7dJ_ivYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 2e-5\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXVTHcYkBPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "    \n",
        "def train_model(model, train_iter, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.cuda()\n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        text = batch.text[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.cuda()\n",
        "            target = target.cuda()\n",
        "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        optim.zero_grad()\n",
        "        prediction,att= model(text)\n",
        "        loss = loss_fn(prediction, target)\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optim.step()\n",
        "        steps += 1\n",
        "        \n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "        \n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "        \n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch.text[0]\n",
        "            if (text.size()[0] is not 32):\n",
        "                continue\n",
        "            target = batch.label\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction,att = model(text)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter),att\n",
        "\t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is6TRILDpthm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(AttentionModel, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t#self.attn_fc_layer = nn.Linear()\n",
        "\t\t\n",
        "\tdef attention_net(self, lstm_output, final_state):\n",
        "\n",
        "\t\t\"\"\" \n",
        "\t\tNow we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n",
        "\t\tbetween each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n",
        "\t\t\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tlstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
        "\t\tfinal_state : Final time-step hidden state (h_n) of the LSTM\n",
        "\t\t\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tReturns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
        "\t\t\t\t  new hidden state.\n",
        "\t\t\t\t  \n",
        "\t\tTensor Size :\n",
        "\t\t\t\t\thidden.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\tattn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tsoft_attn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tnew_hidden_state.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\t  \n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\thidden = final_state.squeeze(0)\n",
        "\t\tattn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "\t\tsoft_attn_weights = F.softmax(attn_weights, 1)\n",
        "\t\tnew_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "\t\t\n",
        "\t\treturn new_hidden_state,soft_attn_weights\n",
        "\t\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\t\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size) \n",
        "\t\toutput = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
        "\t\t\n",
        "\t\tattn_output,attention_weight = self.attention_net(output, final_hidden_state)\n",
        "\t\t#print(attention_weight)\n",
        "\t\tlogits = self.label(attn_output)\n",
        "\t\t\n",
        "\t\treturn logits,attention_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yRdzv6_q6Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmnji1HJrd2x",
        "colab_type": "code",
        "outputId": "3318c945-4c1b-4bdf-ef49-f94b52352179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1108
        }
      },
      "source": [
        "for epoch in range(10):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc,_ = eval_model(model, valid_iter)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        "    \n",
        "test_loss, test_acc,_ = eval_model(model, test_iter)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n",
        "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
        "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
        "\n",
        "test_sen1 = TEXT.preprocess(test_sen1)\n",
        "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
        "\n",
        "test_sen2 = TEXT.preprocess(test_sen2)\n",
        "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
        "\n",
        "test_sen = np.asarray(test_sen1)\n",
        "test_sen = torch.LongTensor(test_sen)\n",
        "test_tensor = Variable(test_sen, volatile=True)\n",
        "test_tensor = test_tensor.cuda()\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Idx: 100, Training Loss: 0.6885, Training Accuracy:  56.25%\n",
            "Epoch: 1, Idx: 200, Training Loss: 0.6958, Training Accuracy:  53.12%\n",
            "Epoch: 1, Idx: 300, Training Loss: 0.6887, Training Accuracy:  62.50%\n",
            "Epoch: 1, Idx: 400, Training Loss: 0.6290, Training Accuracy:  68.75%\n",
            "Epoch: 1, Idx: 500, Training Loss: 0.6521, Training Accuracy:  65.62%\n",
            "Epoch: 01, Train Loss: 0.675, Train Acc: 57.56%, Val. Loss: 0.583864, Val. Acc: 69.72%\n",
            "Epoch: 2, Idx: 100, Training Loss: 0.3394, Training Accuracy:  84.38%\n",
            "Epoch: 2, Idx: 200, Training Loss: 0.5695, Training Accuracy:  75.00%\n",
            "Epoch: 2, Idx: 300, Training Loss: 0.4713, Training Accuracy:  71.88%\n",
            "Epoch: 2, Idx: 400, Training Loss: 0.1711, Training Accuracy:  96.88%\n",
            "Epoch: 2, Idx: 500, Training Loss: 0.3395, Training Accuracy:  84.38%\n",
            "Epoch: 02, Train Loss: 0.417, Train Acc: 81.21%, Val. Loss: 0.428006, Val. Acc: 81.69%\n",
            "Epoch: 3, Idx: 100, Training Loss: 0.1942, Training Accuracy:  84.38%\n",
            "Epoch: 3, Idx: 200, Training Loss: 0.2531, Training Accuracy:  90.62%\n",
            "Epoch: 3, Idx: 300, Training Loss: 0.1946, Training Accuracy:  90.62%\n",
            "Epoch: 3, Idx: 400, Training Loss: 0.2876, Training Accuracy:  81.25%\n",
            "Epoch: 3, Idx: 500, Training Loss: 0.2320, Training Accuracy:  90.62%\n",
            "Epoch: 03, Train Loss: 0.233, Train Acc: 90.86%, Val. Loss: 0.378424, Val. Acc: 83.50%\n",
            "Epoch: 4, Idx: 100, Training Loss: 0.0546, Training Accuracy:  100.00%\n",
            "Epoch: 4, Idx: 200, Training Loss: 0.0106, Training Accuracy:  100.00%\n",
            "Epoch: 4, Idx: 300, Training Loss: 0.0477, Training Accuracy:  96.88%\n",
            "Epoch: 4, Idx: 400, Training Loss: 0.2531, Training Accuracy:  90.62%\n",
            "Epoch: 4, Idx: 500, Training Loss: 0.1202, Training Accuracy:  93.75%\n",
            "Epoch: 04, Train Loss: 0.117, Train Acc: 95.67%, Val. Loss: 0.432514, Val. Acc: 83.79%\n",
            "Epoch: 5, Idx: 100, Training Loss: 0.0110, Training Accuracy:  100.00%\n",
            "Epoch: 5, Idx: 200, Training Loss: 0.1042, Training Accuracy:  93.75%\n",
            "Epoch: 5, Idx: 300, Training Loss: 0.0687, Training Accuracy:  93.75%\n",
            "Epoch: 5, Idx: 400, Training Loss: 0.2918, Training Accuracy:  93.75%\n",
            "Epoch: 5, Idx: 500, Training Loss: 0.1487, Training Accuracy:  93.75%\n",
            "Epoch: 05, Train Loss: 0.045, Train Acc: 98.42%, Val. Loss: 0.590698, Val. Acc: 83.19%\n",
            "Epoch: 6, Idx: 100, Training Loss: 0.0014, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 200, Training Loss: 0.0086, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 300, Training Loss: 0.0193, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 400, Training Loss: 0.0002, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 500, Training Loss: 0.0036, Training Accuracy:  100.00%\n",
            "Epoch: 06, Train Loss: 0.019, Train Acc: 99.26%, Val. Loss: 0.676750, Val. Acc: 83.13%\n",
            "Epoch: 7, Idx: 100, Training Loss: 0.0556, Training Accuracy:  96.88%\n",
            "Epoch: 7, Idx: 200, Training Loss: 0.0022, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 300, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 400, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 500, Training Loss: 0.0037, Training Accuracy:  100.00%\n",
            "Epoch: 07, Train Loss: 0.008, Train Acc: 99.62%, Val. Loss: 0.826686, Val. Acc: 83.29%\n",
            "Epoch: 8, Idx: 100, Training Loss: 0.0025, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 200, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 300, Training Loss: 0.0173, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 400, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 500, Training Loss: 0.0101, Training Accuracy:  100.00%\n",
            "Epoch: 08, Train Loss: 0.006, Train Acc: 99.65%, Val. Loss: 1.046355, Val. Acc: 80.81%\n",
            "Epoch: 9, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 200, Training Loss: 0.0002, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 300, Training Loss: 0.0027, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 400, Training Loss: 0.1456, Training Accuracy:  96.88%\n",
            "Epoch: 9, Idx: 500, Training Loss: 0.0008, Training Accuracy:  100.00%\n",
            "Epoch: 09, Train Loss: 0.007, Train Acc: 99.63%, Val. Loss: 0.916565, Val. Acc: 82.41%\n",
            "Epoch: 10, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 200, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 300, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 400, Training Loss: 0.0002, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 500, Training Loss: 0.0005, Training Accuracy:  100.00%\n",
            "Epoch: 10, Train Loss: 0.003, Train Acc: 99.74%, Val. Loss: 1.099899, Val. Acc: 82.37%\n",
            "Test Loss: 1.104, Test Acc: 80.64%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTkaBwB0kbRW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "791e75f7-f41f-4199-83f7-b27022e1efc2"
      },
      "source": [
        "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
        "test_sen1 = TEXT.preprocess(test_sen1)\n",
        "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
        "\n",
        "test_sen = np.asarray(test_sen1)\n",
        "test_sen = torch.LongTensor(test_sen)\n",
        "test_tensor = Variable(test_sen, volatile=True)\n",
        "test_tensor = test_tensor.cuda()\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVnmeEAq0CR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_result(sentence):\n",
        "  test_sen2 = sentence\n",
        "  test_sen2 = TEXT.preprocess(test_sen2)\n",
        "  tmp_sen2 = test_sen2.copy()\n",
        "  test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
        "\n",
        "  test_sen = np.asarray(test_sen2)\n",
        "  test_sen = torch.LongTensor(test_sen)\n",
        "  test_tensor = Variable(test_sen)\n",
        "  test_tensor = test_tensor.cuda()\n",
        "\n",
        "  output,att = model(test_tensor, 1)\n",
        "  out = F.softmax(output, 1)\n",
        "\n",
        "  reason = tmp_sen2[torch.max(att,1).indices[0].item()]\n",
        "  if (torch.argmax(out[0]) == 1):\n",
        "      print (\"Sentiment: Positive --- Reason: %s\"  % reason) \n",
        "  else:\n",
        "      print (\"Sentiment: Negative --- Reason: %s\"  % reason)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGbo8FqbGs9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "166a7b93-958e-4e98-dce3-db8c43a195e6"
      },
      "source": [
        "sentence = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
        "check_result(sentence)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment: Negative --- Reason: waste\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf1EzDPdGCIj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29bac832-3c5a-42d4-b7b8-38ebc8bdbf11"
      },
      "source": [
        "sentence = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
        "check_result(sentence)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment: Positive --- Reason: dialogues.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}